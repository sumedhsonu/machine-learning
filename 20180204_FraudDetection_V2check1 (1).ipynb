{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_SEED =7124\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data and lookng at the structure of the data. This data has 30 attributes and 1 lakh records\n",
    "data = pd.read_csv(\"Fraud_data_amtstd.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    99508\n",
      "1      492\n",
      "Name: Class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9e7/A+BW2SWyijhRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f92543278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLotting the frequencies of fraud and non-fraud transactions in the data\n",
    "count_classes = pd.value_counts(data['Class'], sort = True)\n",
    "print(count_classes)\n",
    "\n",
    "#Drawing a barplot\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "\n",
    "#Giving titles and labels to the plot\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.836500</td>\n",
       "      <td>-0.545419</td>\n",
       "      <td>-0.462979</td>\n",
       "      <td>0.537174</td>\n",
       "      <td>-0.426143</td>\n",
       "      <td>-0.100606</td>\n",
       "      <td>-0.584764</td>\n",
       "      <td>-0.103956</td>\n",
       "      <td>2.268429</td>\n",
       "      <td>-0.365185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085111</td>\n",
       "      <td>0.410736</td>\n",
       "      <td>0.137625</td>\n",
       "      <td>0.602906</td>\n",
       "      <td>-0.350260</td>\n",
       "      <td>0.464407</td>\n",
       "      <td>-0.070917</td>\n",
       "      <td>-0.030486</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.289880</td>\n",
       "      <td>-2.576061</td>\n",
       "      <td>-0.092256</td>\n",
       "      <td>1.976405</td>\n",
       "      <td>2.810033</td>\n",
       "      <td>-2.669128</td>\n",
       "      <td>-0.981883</td>\n",
       "      <td>-0.470310</td>\n",
       "      <td>-0.025692</td>\n",
       "      <td>0.099528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473240</td>\n",
       "      <td>-0.307295</td>\n",
       "      <td>-2.789549</td>\n",
       "      <td>0.578976</td>\n",
       "      <td>-0.837979</td>\n",
       "      <td>0.372843</td>\n",
       "      <td>0.353451</td>\n",
       "      <td>-1.662202</td>\n",
       "      <td>-0.347171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.131318</td>\n",
       "      <td>0.139818</td>\n",
       "      <td>0.586921</td>\n",
       "      <td>1.069291</td>\n",
       "      <td>-0.334908</td>\n",
       "      <td>-0.204938</td>\n",
       "      <td>-0.135526</td>\n",
       "      <td>0.043821</td>\n",
       "      <td>-0.121117</td>\n",
       "      <td>0.182139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028126</td>\n",
       "      <td>-0.167062</td>\n",
       "      <td>-0.048054</td>\n",
       "      <td>-0.009912</td>\n",
       "      <td>0.417694</td>\n",
       "      <td>-0.479793</td>\n",
       "      <td>0.024360</td>\n",
       "      <td>0.023878</td>\n",
       "      <td>-0.208963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.866956</td>\n",
       "      <td>1.373947</td>\n",
       "      <td>1.948343</td>\n",
       "      <td>2.686750</td>\n",
       "      <td>-0.366790</td>\n",
       "      <td>0.568632</td>\n",
       "      <td>-0.278349</td>\n",
       "      <td>0.739536</td>\n",
       "      <td>-1.655955</td>\n",
       "      <td>0.708396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>-0.070619</td>\n",
       "      <td>-0.080307</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.092167</td>\n",
       "      <td>0.159131</td>\n",
       "      <td>0.157940</td>\n",
       "      <td>-0.014370</td>\n",
       "      <td>-0.253595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.842670</td>\n",
       "      <td>1.401843</td>\n",
       "      <td>0.927235</td>\n",
       "      <td>1.070402</td>\n",
       "      <td>0.843883</td>\n",
       "      <td>0.467333</td>\n",
       "      <td>0.366716</td>\n",
       "      <td>0.616739</td>\n",
       "      <td>-1.586963</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036573</td>\n",
       "      <td>-0.182581</td>\n",
       "      <td>-0.226834</td>\n",
       "      <td>-1.029794</td>\n",
       "      <td>-0.118762</td>\n",
       "      <td>-0.228960</td>\n",
       "      <td>-0.024250</td>\n",
       "      <td>0.046547</td>\n",
       "      <td>-0.346230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.178458</td>\n",
       "      <td>0.166055</td>\n",
       "      <td>-0.101567</td>\n",
       "      <td>0.369453</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>-0.722891</td>\n",
       "      <td>0.396639</td>\n",
       "      <td>-0.187978</td>\n",
       "      <td>-0.483147</td>\n",
       "      <td>0.083094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323048</td>\n",
       "      <td>-1.083814</td>\n",
       "      <td>0.049838</td>\n",
       "      <td>-0.002872</td>\n",
       "      <td>0.295810</td>\n",
       "      <td>0.135883</td>\n",
       "      <td>-0.074191</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>-0.150527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.869017</td>\n",
       "      <td>-0.202287</td>\n",
       "      <td>-0.218739</td>\n",
       "      <td>1.496434</td>\n",
       "      <td>-0.403332</td>\n",
       "      <td>-0.013593</td>\n",
       "      <td>-0.342586</td>\n",
       "      <td>0.129402</td>\n",
       "      <td>0.911017</td>\n",
       "      <td>0.149568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.458963</td>\n",
       "      <td>-1.058509</td>\n",
       "      <td>0.439679</td>\n",
       "      <td>-0.066668</td>\n",
       "      <td>-0.376792</td>\n",
       "      <td>-1.125226</td>\n",
       "      <td>0.053537</td>\n",
       "      <td>-0.039380</td>\n",
       "      <td>-0.305050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.335053</td>\n",
       "      <td>0.331464</td>\n",
       "      <td>-2.057763</td>\n",
       "      <td>-0.346175</td>\n",
       "      <td>2.583234</td>\n",
       "      <td>2.854102</td>\n",
       "      <td>-0.187547</td>\n",
       "      <td>0.685154</td>\n",
       "      <td>-0.286614</td>\n",
       "      <td>-0.535903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191820</td>\n",
       "      <td>-0.650118</td>\n",
       "      <td>-0.114069</td>\n",
       "      <td>0.915936</td>\n",
       "      <td>0.730073</td>\n",
       "      <td>0.383879</td>\n",
       "      <td>-0.031902</td>\n",
       "      <td>0.029849</td>\n",
       "      <td>-0.347171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.787763</td>\n",
       "      <td>-0.737892</td>\n",
       "      <td>-0.185794</td>\n",
       "      <td>0.362758</td>\n",
       "      <td>-0.550775</td>\n",
       "      <td>0.676564</td>\n",
       "      <td>-0.932369</td>\n",
       "      <td>0.390445</td>\n",
       "      <td>1.349983</td>\n",
       "      <td>-0.136095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331025</td>\n",
       "      <td>1.223539</td>\n",
       "      <td>0.264791</td>\n",
       "      <td>-0.541170</td>\n",
       "      <td>-0.571339</td>\n",
       "      <td>0.812785</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>-0.054691</td>\n",
       "      <td>-0.232691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.055540</td>\n",
       "      <td>0.942471</td>\n",
       "      <td>0.986697</td>\n",
       "      <td>1.560551</td>\n",
       "      <td>-0.138755</td>\n",
       "      <td>-0.253645</td>\n",
       "      <td>0.622974</td>\n",
       "      <td>-0.321826</td>\n",
       "      <td>-0.215914</td>\n",
       "      <td>0.816454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153985</td>\n",
       "      <td>0.985031</td>\n",
       "      <td>0.204281</td>\n",
       "      <td>0.455561</td>\n",
       "      <td>-0.456576</td>\n",
       "      <td>-0.244140</td>\n",
       "      <td>-0.833487</td>\n",
       "      <td>-0.419773</td>\n",
       "      <td>-0.173549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  1.836500 -0.545419 -0.462979  0.537174 -0.426143 -0.100606 -0.584764   \n",
       "1 -4.289880 -2.576061 -0.092256  1.976405  2.810033 -2.669128 -0.981883   \n",
       "2  1.131318  0.139818  0.586921  1.069291 -0.334908 -0.204938 -0.135526   \n",
       "3 -0.866956  1.373947  1.948343  2.686750 -0.366790  0.568632 -0.278349   \n",
       "4 -0.842670  1.401843  0.927235  1.070402  0.843883  0.467333  0.366716   \n",
       "5  1.178458  0.166055 -0.101567  0.369453  0.017198 -0.722891  0.396639   \n",
       "6  1.869017 -0.202287 -0.218739  1.496434 -0.403332 -0.013593 -0.342586   \n",
       "7  1.335053  0.331464 -2.057763 -0.346175  2.583234  2.854102 -0.187547   \n",
       "8  1.787763 -0.737892 -0.185794  0.362758 -0.550775  0.676564 -0.932369   \n",
       "9 -1.055540  0.942471  0.986697  1.560551 -0.138755 -0.253645  0.622974   \n",
       "\n",
       "         V8        V9       V10  ...         V21       V22       V23  \\\n",
       "0 -0.103956  2.268429 -0.365185  ...    0.085111  0.410736  0.137625   \n",
       "1 -0.470310 -0.025692  0.099528  ...   -0.473240 -0.307295 -2.789549   \n",
       "2  0.043821 -0.121117  0.182139  ...   -0.028126 -0.167062 -0.048054   \n",
       "3  0.739536 -1.655955  0.708396  ...    0.022719 -0.070619 -0.080307   \n",
       "4  0.616739 -1.586963  0.000041  ...    0.036573 -0.182581 -0.226834   \n",
       "5 -0.187978 -0.483147  0.083094  ...   -0.323048 -1.083814  0.049838   \n",
       "6  0.129402  0.911017  0.149568  ...   -0.458963 -1.058509  0.439679   \n",
       "7  0.685154 -0.286614 -0.535903  ...   -0.191820 -0.650118 -0.114069   \n",
       "8  0.390445  1.349983 -0.136095  ...    0.331025  1.223539  0.264791   \n",
       "9 -0.321826 -0.215914  0.816454  ...    0.153985  0.985031  0.204281   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  Class  \n",
       "0  0.602906 -0.350260  0.464407 -0.070917 -0.030486  0.049882      0  \n",
       "1  0.578976 -0.837979  0.372843  0.353451 -1.662202 -0.347171      0  \n",
       "2 -0.009912  0.417694 -0.479793  0.024360  0.023878 -0.208963      0  \n",
       "3  0.000816  0.092167  0.159131  0.157940 -0.014370 -0.253595      0  \n",
       "4 -1.029794 -0.118762 -0.228960 -0.024250  0.046547 -0.346230      0  \n",
       "5 -0.002872  0.295810  0.135883 -0.074191  0.004364 -0.150527      0  \n",
       "6 -0.066668 -0.376792 -1.125226  0.053537 -0.039380 -0.305050      0  \n",
       "7  0.915936  0.730073  0.383879 -0.031902  0.029849 -0.347171      0  \n",
       "8 -0.541170 -0.571339  0.812785  0.017984 -0.054691 -0.232691      0  \n",
       "9  0.455561 -0.456576 -0.244140 -0.833487 -0.419773 -0.173549      0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at a sample of records\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Converting data to array\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 30)\n",
      "(20000, 30)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into train and test and observing their dimensions\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.,  1.]), array([79591,   409], dtype=int64))\n",
      "(array([ 0.,  1.]), array([19917,    83], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the fraud and non-fraud records in train\n",
    "print(np.unique(X_train[:,29],return_counts=True))\n",
    "print(np.unique(X_test[:,29],return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79591, 29)\n"
     ]
    }
   ],
   "source": [
    "#Now consider only the non-fraud records for training\n",
    "X_train1 = X_train[X_train[:,-1] == 0]\n",
    "X_train1 = X_train1[:,:-1]\n",
    "print(X_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 30)\n"
     ]
    }
   ],
   "source": [
    "#Separating out the fraud records from the train \n",
    "X_train2 = X_train[X_train[:,-1] == 1]\n",
    "print(X_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79591, 29)\n"
     ]
    }
   ],
   "source": [
    "X_train=X_train1\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20409, 30)\n"
     ]
    }
   ],
   "source": [
    "#Adding/concatinating the fraud records from train data to the test\n",
    "X_test=np.concatenate((X_test,X_train2),axis=0)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test,X_eval = train_test_split(X_test, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16327, 30)\n",
      "(4082, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separating the independent and the class variable\n",
    "y_test = X_test[:,-1]\n",
    "X_test = X_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16327, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expanding the dimensions of y for later concatenation\n",
    "y_test = np.expand_dims(y_test, axis=1)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16327, 30)\n",
      "(15942, 30)\n",
      "(385, 30)\n"
     ]
    }
   ],
   "source": [
    "##We want to separate out fraud records and non-fraud records for later use\n",
    "f = np.hstack((X_test,y_test))\n",
    "print(f.shape)\n",
    "\n",
    "test_nf=f[f[:,29]==0]\n",
    "print(test_nf.shape)\n",
    "\n",
    "test_f=f[f[:,29]==1]\n",
    "print(test_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Removing unnecessary variables\n",
    "del X_train1, X_train2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder = Sequential()\n",
    "\n",
    "autoencoder.add(Dropout(0.2, input_shape=(input_dim,)))\n",
    "#autoencoder.add(Dense(20, activation='sigmoid'))\n",
    "autoencoder.add(Dense(encoding_dim, activation='relu'))\n",
    "#autoencoder.add(Dense(20, activation='sigmoid'))\n",
    "autoencoder.add(Dense(input_dim, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.5861 - mean_squared_error: 0.5861 - val_loss: 0.7691 - val_mean_squared_error: 0.7691\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3705 - mean_squared_error: 0.3705 - val_loss: 0.7021 - val_mean_squared_error: 0.7021\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3561 - mean_squared_error: 0.3561 - val_loss: 0.7034 - val_mean_squared_error: 0.7034\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3519 - mean_squared_error: 0.3519 - val_loss: 0.7168 - val_mean_squared_error: 0.7168\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3525 - mean_squared_error: 0.3525 - val_loss: 0.6919 - val_mean_squared_error: 0.6919\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3468 - mean_squared_error: 0.3468 - val_loss: 0.7063 - val_mean_squared_error: 0.7063\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3461 - mean_squared_error: 0.3461 - val_loss: 0.7066 - val_mean_squared_error: 0.7066\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3467 - mean_squared_error: 0.3467 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3412 - mean_squared_error: 0.3412 - val_loss: 0.7031 - val_mean_squared_error: 0.7031\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3435 - mean_squared_error: 0.3435 - val_loss: 0.7127 - val_mean_squared_error: 0.7127\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3414 - mean_squared_error: 0.3414 - val_loss: 0.7002 - val_mean_squared_error: 0.7002\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3455 - mean_squared_error: 0.3455 - val_loss: 0.6813 - val_mean_squared_error: 0.6813\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3396 - mean_squared_error: 0.3396 - val_loss: 0.6905 - val_mean_squared_error: 0.6905\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3420 - mean_squared_error: 0.3420 - val_loss: 0.6926 - val_mean_squared_error: 0.6926\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3382 - mean_squared_error: 0.3382 - val_loss: 0.6984 - val_mean_squared_error: 0.6984\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3400 - mean_squared_error: 0.3400 - val_loss: 0.7066 - val_mean_squared_error: 0.7066\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3371 - mean_squared_error: 0.3371 - val_loss: 0.6934 - val_mean_squared_error: 0.6934\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3393 - mean_squared_error: 0.3393 - val_loss: 0.6866 - val_mean_squared_error: 0.6866\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3354 - mean_squared_error: 0.3354 - val_loss: 0.6732 - val_mean_squared_error: 0.6732\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3360 - mean_squared_error: 0.3360 - val_loss: 0.6840 - val_mean_squared_error: 0.6840\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3366 - mean_squared_error: 0.3366 - val_loss: 0.6910 - val_mean_squared_error: 0.6910\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3360 - mean_squared_error: 0.3360 - val_loss: 0.6901 - val_mean_squared_error: 0.6901\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3339 - mean_squared_error: 0.3339 - val_loss: 0.7085 - val_mean_squared_error: 0.7085\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3318 - mean_squared_error: 0.3318 - val_loss: 0.6952 - val_mean_squared_error: 0.6952\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3329 - mean_squared_error: 0.3329 - val_loss: 0.6876 - val_mean_squared_error: 0.6876\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3344 - mean_squared_error: 0.3344 - val_loss: 0.6949 - val_mean_squared_error: 0.6949\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3342 - mean_squared_error: 0.3342 - val_loss: 0.7093 - val_mean_squared_error: 0.7093\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3312 - mean_squared_error: 0.3312 - val_loss: 0.6963 - val_mean_squared_error: 0.6963\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3331 - mean_squared_error: 0.3331 - val_loss: 0.6964 - val_mean_squared_error: 0.6964\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3315 - mean_squared_error: 0.3315 - val_loss: 0.6817 - val_mean_squared_error: 0.6817\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3310 - mean_squared_error: 0.3310 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3343 - mean_squared_error: 0.3343 - val_loss: 0.6937 - val_mean_squared_error: 0.6937\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3285 - mean_squared_error: 0.3285 - val_loss: 0.6742 - val_mean_squared_error: 0.6742\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3299 - mean_squared_error: 0.3299 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3318 - mean_squared_error: 0.3318 - val_loss: 0.6949 - val_mean_squared_error: 0.6949\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3307 - mean_squared_error: 0.3307 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3292 - mean_squared_error: 0.3292 - val_loss: 0.6917 - val_mean_squared_error: 0.6917\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3307 - mean_squared_error: 0.3307 - val_loss: 0.6907 - val_mean_squared_error: 0.6907\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.7033 - val_mean_squared_error: 0.7033\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3272 - mean_squared_error: 0.3272 - val_loss: 0.7075 - val_mean_squared_error: 0.7075\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3333 - mean_squared_error: 0.3333 - val_loss: 0.7050 - val_mean_squared_error: 0.7050\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3299 - mean_squared_error: 0.3299 - val_loss: 0.6954 - val_mean_squared_error: 0.6954\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3286 - mean_squared_error: 0.3286 - val_loss: 0.6860 - val_mean_squared_error: 0.6860\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3307 - mean_squared_error: 0.3307 - val_loss: 0.6655 - val_mean_squared_error: 0.6655\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3308 - mean_squared_error: 0.3308 - val_loss: 0.6771 - val_mean_squared_error: 0.6771\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.6957 - val_mean_squared_error: 0.6957\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3279 - mean_squared_error: 0.3279 - val_loss: 0.6914 - val_mean_squared_error: 0.6914\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3278 - mean_squared_error: 0.3278 - val_loss: 0.6898 - val_mean_squared_error: 0.6898\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3292 - mean_squared_error: 0.3292 - val_loss: 0.6823 - val_mean_squared_error: 0.6823\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3271 - mean_squared_error: 0.3271 - val_loss: 0.6654 - val_mean_squared_error: 0.6654\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3274 - mean_squared_error: 0.3274 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3268 - mean_squared_error: 0.3268 - val_loss: 0.6684 - val_mean_squared_error: 0.6684\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3292 - mean_squared_error: 0.3292 - val_loss: 0.6910 - val_mean_squared_error: 0.6910\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3264 - mean_squared_error: 0.3264 - val_loss: 0.6599 - val_mean_squared_error: 0.6599\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3286 - mean_squared_error: 0.3286 - val_loss: 0.6758 - val_mean_squared_error: 0.6758\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3277 - mean_squared_error: 0.3277 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3233 - mean_squared_error: 0.3233 - val_loss: 0.6858 - val_mean_squared_error: 0.6858\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.6856 - val_mean_squared_error: 0.6856\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3274 - mean_squared_error: 0.3274 - val_loss: 0.6869 - val_mean_squared_error: 0.6869\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3255 - mean_squared_error: 0.3255 - val_loss: 0.6823 - val_mean_squared_error: 0.6823\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.6751 - val_mean_squared_error: 0.6751\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3291 - mean_squared_error: 0.3291 - val_loss: 0.6660 - val_mean_squared_error: 0.6660\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3255 - mean_squared_error: 0.3255 - val_loss: 0.6799 - val_mean_squared_error: 0.6799\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3292 - mean_squared_error: 0.3292 - val_loss: 0.6909 - val_mean_squared_error: 0.6909\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3279 - mean_squared_error: 0.3279 - val_loss: 0.6582 - val_mean_squared_error: 0.6582\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3266 - mean_squared_error: 0.3266 - val_loss: 0.6716 - val_mean_squared_error: 0.6716\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3245 - mean_squared_error: 0.3245 - val_loss: 0.6677 - val_mean_squared_error: 0.6677\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3291 - mean_squared_error: 0.3291 - val_loss: 0.6976 - val_mean_squared_error: 0.6976\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3243 - mean_squared_error: 0.3243 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3275 - mean_squared_error: 0.3275 - val_loss: 0.6939 - val_mean_squared_error: 0.6939\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3224 - mean_squared_error: 0.3224 - val_loss: 0.6704 - val_mean_squared_error: 0.6704\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3253 - mean_squared_error: 0.3253 - val_loss: 0.6708 - val_mean_squared_error: 0.6708\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 6s - loss: 0.3250 - mean_squared_error: 0.3250 - val_loss: 0.6699 - val_mean_squared_error: 0.6699\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 6s - loss: 0.3273 - mean_squared_error: 0.3273 - val_loss: 0.6822 - val_mean_squared_error: 0.6822\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3246 - mean_squared_error: 0.3246 - val_loss: 0.6811 - val_mean_squared_error: 0.6811\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3263 - mean_squared_error: 0.3263 - val_loss: 0.6698 - val_mean_squared_error: 0.6698\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3276 - mean_squared_error: 0.3276 - val_loss: 0.6758 - val_mean_squared_error: 0.6758\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3301 - mean_squared_error: 0.3301 - val_loss: 0.6751 - val_mean_squared_error: 0.6751\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79591/79591 [==============================] - 5s - loss: 0.3268 - mean_squared_error: 0.3268 - val_loss: 0.6761 - val_mean_squared_error: 0.6761\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3244 - mean_squared_error: 0.3244 - val_loss: 0.6873 - val_mean_squared_error: 0.6873\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3234 - mean_squared_error: 0.3234 - val_loss: 0.6851 - val_mean_squared_error: 0.6851\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3271 - mean_squared_error: 0.3271 - val_loss: 0.6839 - val_mean_squared_error: 0.6839\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3267 - mean_squared_error: 0.3267 - val_loss: 0.6862 - val_mean_squared_error: 0.6862\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3264 - mean_squared_error: 0.3264 - val_loss: 0.6721 - val_mean_squared_error: 0.6721\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3249 - mean_squared_error: 0.3249 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3246 - mean_squared_error: 0.3246 - val_loss: 0.6738 - val_mean_squared_error: 0.6738\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3264 - mean_squared_error: 0.3264 - val_loss: 0.6680 - val_mean_squared_error: 0.6680\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3250 - mean_squared_error: 0.3250 - val_loss: 0.6960 - val_mean_squared_error: 0.6960\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3259 - mean_squared_error: 0.3259 - val_loss: 0.6931 - val_mean_squared_error: 0.6931\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3253 - mean_squared_error: 0.3253 - val_loss: 0.6954 - val_mean_squared_error: 0.6954\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3244 - mean_squared_error: 0.3244 - val_loss: 0.6753 - val_mean_squared_error: 0.6753\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3236 - mean_squared_error: 0.3236 - val_loss: 0.6807 - val_mean_squared_error: 0.6807\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.7011 - val_mean_squared_error: 0.7011\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.6721 - val_mean_squared_error: 0.6721\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3268 - mean_squared_error: 0.3268 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3259 - mean_squared_error: 0.3259 - val_loss: 0.6855 - val_mean_squared_error: 0.6855\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.6764 - val_mean_squared_error: 0.6764\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 4s - loss: 0.3219 - mean_squared_error: 0.3219 - val_loss: 0.6828 - val_mean_squared_error: 0.6828\n",
      "Train on 79591 samples, validate on 16327 samples\n",
      "Epoch 1/1\n",
      "79591/79591 [==============================] - 5s - loss: 0.3275 - mean_squared_error: 0.3275 - val_loss: 0.6775 - val_mean_squared_error: 0.6775\n"
     ]
    }
   ],
   "source": [
    "hist = []\n",
    "for _ in range(100):\n",
    "    hist.append(autoencoder.fit(X_train, X_train,\n",
    "                    epochs=1,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1).history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_hist_ = [(x['loss'][0], x['val_loss'][0]) for x in hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##To extract outputs from the hidden layer\n",
    "from keras import backend as K\n",
    "layer_output_encoded = K.function([autoencoder.layers[0].input,K.learning_phase()],\n",
    "                                  [autoencoder.layers[1].output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded=layer_output_encoded([X_train,0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##To extract outputs from the output layer\n",
    "layer_output_decoded = K.function([autoencoder.layers[0].input, K.learning_phase()],\n",
    "                                  [autoencoder.layers[2].output])\n",
    "decoded = layer_output_decoded([X_train, 0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Making predictions on the train data\n",
    "predictions=autoencoder.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14560/15942 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2329422312079103, 0.2329422312079103]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the errors from the non fraud data separately \n",
    "autoencoder.evaluate(test_nf[:,:29],test_nf[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/385 [=>............................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.086169242858887, 19.086169242858887]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the errors from the fraud data separately\n",
    "autoencoder.evaluate(test_f[:,:29],test_f[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtaining predictions for non fraud records\n",
    "predictions_nf=autoencoder.predict(test_nf[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtaining preictions for fraud records\n",
    "predictions_f=autoencoder.predict(test_f[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23294223062716882"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying the error computation method by autoencoder(Mean Squared Error). The computation is as follows \n",
    "np.mean(np.square(np.abs(test_nf[:,:29]-predictions_nf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing errors on the non-fraud data\n",
    "errors_nf = np.mean(np.square(np.abs(test_nf[:,:29]-predictions_nf)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing errors on the fraud data\n",
    "errors_f = np.mean(np.square(np.abs(test_f[:,:29]-predictions_f)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0127176941504\n",
      "92.1335036882\n",
      "0.143308296747\n",
      "0.0485164909068\n",
      "134.096931965\n",
      "7.75419593381\n"
     ]
    }
   ],
   "source": [
    "#Computing the distribution of errors in both non-fraud and fraud data\n",
    "print(np.min(errors_nf))\n",
    "print(np.max(errors_nf))\n",
    "print(np.median(errors_nf))\n",
    "\n",
    "print(np.min(errors_f))\n",
    "print(np.max(errors_f))\n",
    "print(np.median(errors_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': [<matplotlib.lines.Line2D at 0x17fa2710fd0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x17fa27149e8>,\n",
       "  <matplotlib.lines.Line2D at 0x17fa2714be0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x17fa270bcc0>],\n",
       " 'means': [],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x17fa270b470>],\n",
       " 'whiskers': [<matplotlib.lines.Line2D at 0x17fa2713908>,\n",
       "  <matplotlib.lines.Line2D at 0x17fa2713b38>]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIIAAARhCAYAAABeRS5AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntFdbvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17fa3691ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLotting the error box plots \n",
    "size=(20,20)\n",
    "fig,ax = plt.subplots(figsize=size)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(errors_f)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylim(0,1)\n",
    "plt.boxplot(errors_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "192\n",
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7971"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Experimentation to fix a threshold for classification of a transaction into fraud or non-fraud\n",
    "print(sum(errors_nf>np.median(errors_f)))\n",
    "print(sum(errors_f<np.median(errors_f)))\n",
    "print(sum(errors_f<np.median(errors_nf)))\n",
    "sum(errors_nf>np.median(errors_nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15942,)\n",
      "(385,)\n"
     ]
    }
   ],
   "source": [
    "print(errors_nf.shape)\n",
    "print(errors_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15942, 29)\n",
      "(385, 29)\n"
     ]
    }
   ],
   "source": [
    "print(predictions_nf.shape)\n",
    "print(predictions_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = autoencoder.predict(X_test[:,:29])\n",
    "test_recon  = (((test_pred-X_test)**2).mean(-1))\n",
    "\n",
    "train_pred = autoencoder.predict(X_train[:,:29])\n",
    "mean_recon = (((train_pred - X_train)**2).mean(-1).mean())\n",
    "\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n",
    "\n",
    "scores_f1 = []\n",
    "thres = []\n",
    "\n",
    "th = 0\n",
    "for i in range(100):\n",
    "    th+=0.1\n",
    "    fraud = (test_recon>mean_recon+th)\n",
    "    scores_f1.append(f1_score(y_test,fraud))\n",
    "    thres.append(th+mean_recon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.82945941313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[15890,    52],\n",
       "       [   89,   296]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf7u5UeMO/IRHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f9ba5fac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thres, scores_f1)\n",
    "\n",
    "print(thres[np.array(scores_f1).argmax()])\n",
    "\n",
    "fraud = (test_recon>thres[np.array(scores_f1).argmax()])\n",
    "\n",
    "confusion_matrix(y_test, fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_pred = autoencoder.predict(X_eval[:,:29])\n",
    "eval_recon  = (((eval_pred-X_eval[:,:29])**2).mean(-1))\n",
    "fraud_eval = (eval_recon>thres[np.array(scores_f1).argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3968,    7],\n",
       "       [  21,   86]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(X_eval[:,29], fraud_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803738317757\n",
      "0.924731182796\n",
      "0.993140617344\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(X_eval[:,29],fraud_eval))\n",
    "print(precision_score(X_eval[:,29],fraud_eval))\n",
    "print(accuracy_score(X_eval[:,29],fraud_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794805194805\n",
      "0.990291262136\n",
      "0.994977644393\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(y_test, fraud))\n",
    "print(precision_score(y_test, fraud))\n",
    "print(accuracy_score(y_test, fraud))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
